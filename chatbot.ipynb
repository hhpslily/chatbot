{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n",
      "\n",
      "Number of lines:  304714\n"
     ]
    }
   ],
   "source": [
    "# load lines dictionary \n",
    "lines = open('chatbot/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "# load conversations\n",
    "convs_lines = open('chatbot/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "print('\\n'.join(lines[:3]))\n",
    "print()\n",
    "print('\\n'.join(convs_lines[:3]))\n",
    "print()\n",
    "print(\"Number of lines: \", len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They do not!\n"
     ]
    }
   ],
   "source": [
    "# build a id-line dictionary\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]\n",
    "\n",
    "print(id2line['L1045'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L194', 'L195', 'L196', 'L197']\n"
     ]
    }
   ],
   "source": [
    "# build a list of all conversations\n",
    "convs = []\n",
    "for line in convs_lines:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))\n",
    "\n",
    "print(convs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將兩兩連續的對話組成Q-A Pair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "A:  Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Q:  Not the hacking and gagging and spitting part.  Please.\n",
      "A:  Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "Number of questions:  221616\n",
      "Number of answers:  221616\n"
     ]
    }
   ],
   "source": [
    "# make Q-A pair\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])\n",
    "        \n",
    "for i in range(1, 3):\n",
    "    print(\"Q: \", questions[i])\n",
    "    print(\"A: \", answers[i])\n",
    "    print()\n",
    "\n",
    "print(\"Number of questions: \", len(questions))\n",
    "print(\"Number of answers: \", len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "原先有嘗試使用spacy來做word embedding，後來因為出現型別轉換的問題，而直接採用 word-to-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "\n",
    "#nlp = spacy.load('en_core_web_md-2.0.0')\n",
    "\n",
    "# word_apple = nlp('apple')\n",
    "# word_banana = nlp('banana')\n",
    "# word_mac = nlp('mac')\n",
    "\n",
    "# print('%s vs %s: %.6f'%(word_apple, word_banana, word_apple.similarity(word_banana)))\n",
    "# print('%s vs %s: %.6f'%(word_apple, word_mac, word_apple.similarity(word_mac)))\n",
    "# print('%s vs %s: %.6f'%(word_banana, word_mac, word_banana.similarity(word_mac)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "1. 全部轉成小寫\n",
    "2. 濾掉標點符號 (,.!?.'#@ 等等)\n",
    "3. 還原連音字 (I'm -> I am, we'd -> we would)\n",
    "4. 只保留長度在2~20之間的對話\n",
    "5. 每句話前面加上SOS、後面加上EOS\n",
    "6. 頻率過低的字用UNK取代\n",
    "7. word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_line(line):\n",
    "    \n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\'d\", \" would\", line)\n",
    "    line = re.sub(r\"i'm\", \"i am\", line)\n",
    "    line = re.sub(r\"he's\", \"he is\", line)\n",
    "    line = re.sub(r\"she's\", \"she is\", line)\n",
    "    line = re.sub(r\"it's\", \"it is\", line)\n",
    "    line = re.sub(r\"that's\", \"that is\", line)\n",
    "    line = re.sub(r\"what's\", \"what is\", line)\n",
    "    line = re.sub(r\"where's\", \"where is\", line)\n",
    "    line = re.sub(r\"how's\", \"how is\", line)\n",
    "    line = re.sub(r\"\\'re\", \" are\", line)\n",
    "    line = re.sub(r\"\\'ll\", \" will\", line)\n",
    "    line = re.sub(r\"\\'d\", \" would\", line)\n",
    "    line = re.sub(r\"n't\", \" not\", line)\n",
    "    line = re.sub(r\"'til\", \"until\", line)\n",
    "    line = re.sub(r\"\\'ve\", \" have\", line)\n",
    "    line = re.sub(r\"won't\", \"will not\", line)\n",
    "    line = re.sub(r\"can't\", \"can not\", line)\n",
    "    line = re.sub(r\"'bout\", \"about\", line)\n",
    "    line = re.sub(r\"n'\", \"and\", line)\n",
    "    line = re.sub(r\"[-()\\\"#/@$;:<>{}`+=~|.!?,]\", \"\", line)\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out punctuation\n",
    "questions_filtered = []\n",
    "for question in questions:\n",
    "    questions_filtered.append(filter_line(question))\n",
    "    \n",
    "answers_filtered = []\n",
    "for answer in answers:\n",
    "    answers_filtered.append(filter_line(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameron.\n",
      "cameron\n",
      "\n",
      "C'esc ma tete. This is my head\n",
      "c'esc ma tete this is my head\n",
      "\n",
      "See that?  Who needs affection when I've got blind hatred?\n",
      "see that  who needs affection when i have got blind hatred\n",
      "\n",
      "I don't want you to wait for me.\n",
      "i do not want you to wait for me\n",
      "\n",
      "Number of questions:  221616\n",
      "Number of answers:  221616\n"
     ]
    }
   ],
   "source": [
    "print(questions[5])\n",
    "print(questions_filtered[5])\n",
    "print()\n",
    "print(questions[10])\n",
    "print(questions_filtered[10])\n",
    "print()\n",
    "print(answers[300])\n",
    "print(answers_filtered[300])\n",
    "print()\n",
    "print(answers[500])\n",
    "print(answers_filtered[500])\n",
    "print()\n",
    "print(\"Number of questions: \", len(questions_filtered))\n",
    "print(\"Number of answers: \", len(answers_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out string with length <3 or >20\n",
    "questions_shorten = []\n",
    "answers_shorten = []\n",
    "for question, answer in zip(questions_filtered, answers_filtered):\n",
    "    q_len = len(question.split())\n",
    "    a_len = len(answer.split())\n",
    "    if q_len >= 2 and q_len <= 20 and a_len >= 2 and a_len <= 20:\n",
    "        questions_shorten.append(question)\n",
    "        answers_shorten.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions:  138164\n",
      "Number of questions:  138164\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of questions: \", len(questions_shorten))\n",
    "print(\"Number of questions: \", len(answers_shorten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "\n",
    "for question, answer in zip(questions_shorten, answers_shorten):\n",
    "    for q in question.split():\n",
    "        if q not in vocab:\n",
    "            vocab[q] = 1\n",
    "        else:\n",
    "            vocab[q] += 1\n",
    "            \n",
    "    for a in answer.split():\n",
    "        if a not in vocab:\n",
    "            vocab[a] = 1\n",
    "        else:\n",
    "            vocab[a] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45911\n",
      "701\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(vocab['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word embedding\n",
    "word2index = {'<SOS>': 0, '<EOS>': 1, '<UNK>': 2, '<PAD>':3}\n",
    "index2word = {0: '<SOS>', 1: '<EOS>', 2: '<UNK>', 3: '<PAD>'}\n",
    "\n",
    "for k, v in vocab.items():\n",
    "    if v >= 10:\n",
    "        word2index[k] = len(index2word)\n",
    "        index2word[len(index2word)] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8123\n",
      "8123\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2index)\n",
    "print(len(word2index))\n",
    "print(len(index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS>\n",
      "well\n"
     ]
    }
   ],
   "source": [
    "print(index2word[0])\n",
    "print(index2word[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace words that are not in the Vocab with <UNK>\n",
    "# add <SOS> and <EOS>\n",
    "\n",
    "Questions = []\n",
    "Answers = []\n",
    "\n",
    "for question, answer in zip(questions_shorten, answers_shorten):\n",
    "    flag = True\n",
    "    tmp_q = []\n",
    "    tmp_a = []\n",
    "    tmp_q.append(word2index['<SOS>'])\n",
    "    tmp_a.append(word2index['<SOS>'])\n",
    "\n",
    "    for q in question.split():\n",
    "        if q not in word2index:\n",
    "            tmp_q.append(word2index['<UNK>'])\n",
    "        else:\n",
    "            tmp_q.append(word2index[q])\n",
    "            \n",
    "    for a in answer.split():\n",
    "        if a not in word2index:\n",
    "            tmp_a.append(word2index['<UNK>'])\n",
    "        else:\n",
    "            tmp_a.append(word2index[a])\n",
    "               \n",
    "    tmp_q.append(word2index['<EOS>'])\n",
    "    tmp_a.append(word2index['<EOS>'])\n",
    "\n",
    "    Questions.append(tmp_q)\n",
    "    Answers.append(tmp_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138164\n",
      "138164\n"
     ]
    }
   ],
   "source": [
    "print(len(Questions))\n",
    "print(len(Answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n"
     ]
    }
   ],
   "source": [
    "# caculate max length\n",
    "q_max_len = 0\n",
    "a_max_len = 0\n",
    "\n",
    "for i in range(len(Questions)):\n",
    "    q_max_len = max(q_max_len, len(Questions[i]))\n",
    "    a_max_len = max(a_max_len, len(Answers[i]))\n",
    "\n",
    "print(q_max_len, a_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, en_corpus, ch_corpus, en_pad, ch_pad, en_max_len, ch_max_len, batch_size):\n",
    "        assert len(en_corpus) == len(ch_corpus)\n",
    "        \n",
    "        batch_num = len(en_corpus)//batch_size\n",
    "        n = batch_num*batch_size\n",
    "        \n",
    "        self.xs = [np.zeros(n, dtype=np.int32) for _ in range(en_max_len)] # encoder inputs\n",
    "        self.ys = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder inputs\n",
    "        self.gs = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder outputs\n",
    "        self.ws = [np.zeros(n, dtype=np.float32) for _ in range(ch_max_len)] # decoder weight for loss caculation\n",
    "        \n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for b in range(batch_num):\n",
    "            for i in range(b*batch_size, (b+1)*batch_size):\n",
    "                for j in range(len(en_corpus[i])-2):\n",
    "                    self.xs[j][i] = en_corpus[i][j+1]\n",
    "                for j in range(j+1, en_max_len):\n",
    "                    self.xs[j][i] = en_pad\n",
    "                \n",
    "                for j in range(len(ch_corpus[i])-1):\n",
    "                    self.ys[j][i] = ch_corpus[i][j]\n",
    "                    self.gs[j][i] = ch_corpus[i][j+1]\n",
    "                    self.ws[j][i] = 1.0\n",
    "                for j in range(j+1, ch_max_len): # don't forget padding and let loss weight zero\n",
    "                    self.ys[j][i] = ch_pad\n",
    "                    self.gs[j][i] = ch_pad\n",
    "                    self.ws[j][i] = 0.0\n",
    "    \n",
    "    def get(self, batch_id):\n",
    "        x = [self.xs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.en_max_len)]\n",
    "        y = [self.ys[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        g = [self.gs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        w = [self.ws[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        \n",
    "        return x, y, g, w\n",
    "\n",
    "\n",
    "batch = BatchGenerator(Questions, Answers, word2index['<PAD>'], word2index['<PAD>'], q_max_len, a_max_len, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq:\n",
    "    def __init__(self, en_max_len, ch_max_len, en_size, ch_size):\n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        \n",
    "        with tf.variable_scope('seq2seq_intput/output'):\n",
    "            self.enc_inputs = [tf.placeholder(tf.int32, [None]) for i in range(en_max_len)] # time mojor feed\n",
    "            self.dec_inputs = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.groundtruths = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.weights = [tf.placeholder(tf.float32, [None]) for i in range(ch_max_len)]\n",
    "            \n",
    "        with tf.variable_scope('seq2seq_rnn'): # training by teacher forcing\n",
    "            self.out_cell = tf.contrib.rnn.LSTMCell(512)\n",
    "            self.outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                    self.out_cell, \n",
    "                                                                                    en_size, ch_size, 300)\n",
    "        with tf.variable_scope('seq2seq_rnn', reuse=True): # predict by feeding previous\n",
    "            self.pred_cell = tf.contrib.rnn.LSTMCell(512, reuse=True) # reuse cell for train and test\n",
    "            self.predictions, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                        self.pred_cell, \n",
    "                                                                                        en_size, ch_size, 300, \n",
    "                                                                                        feed_previous=True)\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            # caculate weighted loss\n",
    "            self.loss = tf.reduce_mean(tf.contrib.legacy_seq2seq.sequence_loss_by_example(self.outputs, \n",
    "                                                                                          self.groundtruths, \n",
    "                                                                                          self.weights))\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.002).minimize(self.loss)\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train(self, x, y, g, w):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i] # show how to feed a list\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "            fd[self.groundtruths[i]] = g[i]\n",
    "            fd[self.weights[i]] = w[i]\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss, self.optimizer], fd)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def output(self, x, y):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "        \n",
    "        out = self.sess.run(self.outputs, fd)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, x, ch_beg):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len): # when feed previous, the fist token should be '<BEG>', and others are useless\n",
    "            if i==0:\n",
    "                fd[self.dec_inputs[i]] = np.ones(y[i].shape, dtype=np.int32)*ch_beg\n",
    "            else:\n",
    "                fd[self.dec_inputs[i]] = np.zeros(y[i].shape, dtype=np.int32)\n",
    "        \n",
    "        pd = self.sess.run(self.predictions, fd)\n",
    "        \n",
    "        return pd\n",
    "    \n",
    "    def save(self, e):\n",
    "        self.saver.save(self.sess, 'model/seq2seq_%d.ckpt'%(e+1))\n",
    "    \n",
    "    def restore(self, e):\n",
    "        self.saver.restore(self.sess, 'model/seq2seq_%d.ckpt'%(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = seq2seq(q_max_len, a_max_len, len(word2index), len(index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "batch_num = len(Questions) // BATCH_SIZE\n",
    "\n",
    "batch = BatchGenerator(Questions, Answers, word2index['<PAD>'], word2index['<PAD>'], q_max_len, a_max_len, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 loss: 9.008163\n",
      "Batch: 101 loss: 5.568803\n",
      "Batch: 201 loss: 4.995028\n",
      "Batch: 301 loss: 4.135249\n",
      "Batch: 401 loss: 4.420690\n",
      "Batch: 501 loss: 4.450699\n",
      "Batch: 601 loss: 4.623874\n",
      "Batch: 701 loss: 4.597316\n",
      "Batch: 801 loss: 4.541345\n",
      "Batch: 901 loss: 4.701380\n",
      "Batch: 1001 loss: 4.320860\n",
      "epoch 0 loss: 4.723568\n",
      "Batch: 1 loss: 4.398059\n",
      "Batch: 101 loss: 4.577847\n",
      "Batch: 201 loss: 4.474986\n",
      "Batch: 301 loss: 3.641847\n",
      "Batch: 401 loss: 4.001675\n",
      "Batch: 501 loss: 4.137747\n",
      "Batch: 601 loss: 4.261307\n",
      "Batch: 701 loss: 4.332678\n",
      "Batch: 801 loss: 4.228455\n",
      "Batch: 901 loss: 4.426671\n",
      "Batch: 1001 loss: 3.967466\n",
      "epoch 1 loss: 4.221116\n",
      "Batch: 1 loss: 4.187247\n",
      "Batch: 101 loss: 4.155052\n",
      "Batch: 201 loss: 4.199736\n",
      "Batch: 301 loss: 3.416176\n",
      "Batch: 401 loss: 3.791955\n",
      "Batch: 501 loss: 3.967443\n",
      "Batch: 601 loss: 3.929487\n",
      "Batch: 701 loss: 4.098811\n",
      "Batch: 801 loss: 3.975438\n",
      "Batch: 901 loss: 4.166307\n",
      "Batch: 1001 loss: 3.651073\n",
      "epoch 2 loss: 3.956894\n",
      "Batch: 1 loss: 3.967913\n",
      "Batch: 101 loss: 3.623804\n",
      "Batch: 201 loss: 3.879039\n",
      "Batch: 301 loss: 3.266762\n",
      "Batch: 401 loss: 3.569542\n",
      "Batch: 501 loss: 3.741092\n",
      "Batch: 601 loss: 3.552261\n",
      "Batch: 701 loss: 3.839823\n",
      "Batch: 801 loss: 3.678247\n",
      "Batch: 901 loss: 3.891928\n",
      "Batch: 1001 loss: 3.392426\n",
      "epoch 3 loss: 3.695737\n",
      "Batch: 1 loss: 3.672687\n",
      "Batch: 101 loss: 3.290387\n",
      "Batch: 201 loss: 3.616607\n",
      "Batch: 301 loss: 3.103880\n",
      "Batch: 401 loss: 3.347700\n",
      "Batch: 501 loss: 3.518770\n",
      "Batch: 601 loss: 3.297420\n",
      "Batch: 701 loss: 3.567939\n",
      "Batch: 801 loss: 3.359713\n",
      "Batch: 901 loss: 3.618918\n",
      "Batch: 1001 loss: 3.139480\n",
      "epoch 4 loss: 3.444498\n",
      "Batch: 1 loss: 3.393462\n",
      "Batch: 101 loss: 2.924049\n",
      "Batch: 201 loss: 3.324106\n",
      "Batch: 301 loss: 3.014137\n",
      "Batch: 401 loss: 3.188758\n",
      "Batch: 501 loss: 3.306763\n",
      "Batch: 601 loss: 3.039454\n",
      "Batch: 701 loss: 3.335041\n",
      "Batch: 801 loss: 3.138317\n",
      "Batch: 901 loss: 3.401654\n",
      "Batch: 1001 loss: 2.934398\n",
      "epoch 5 loss: 3.228900\n",
      "Batch: 1 loss: 3.236846\n",
      "Batch: 101 loss: 2.684601\n",
      "Batch: 201 loss: 3.105234\n",
      "Batch: 301 loss: 2.858011\n",
      "Batch: 401 loss: 2.995832\n",
      "Batch: 501 loss: 3.154175\n",
      "Batch: 601 loss: 2.837329\n",
      "Batch: 701 loss: 3.088205\n",
      "Batch: 801 loss: 2.942695\n",
      "Batch: 901 loss: 3.232934\n",
      "Batch: 1001 loss: 2.789294\n",
      "epoch 6 loss: 3.050199\n",
      "Batch: 1 loss: 3.050100\n",
      "Batch: 101 loss: 2.503577\n",
      "Batch: 201 loss: 2.962768\n",
      "Batch: 301 loss: 2.753010\n",
      "Batch: 401 loss: 2.858996\n",
      "Batch: 501 loss: 3.042753\n",
      "Batch: 601 loss: 2.624951\n",
      "Batch: 701 loss: 3.009125\n",
      "Batch: 801 loss: 2.777847\n",
      "Batch: 901 loss: 3.166525\n",
      "Batch: 1001 loss: 2.626474\n",
      "epoch 7 loss: 2.901834\n",
      "Batch: 1 loss: 2.917304\n",
      "Batch: 101 loss: 2.298090\n",
      "Batch: 201 loss: 2.779782\n",
      "Batch: 301 loss: 2.669438\n",
      "Batch: 401 loss: 2.720505\n",
      "Batch: 501 loss: 2.859179\n",
      "Batch: 601 loss: 2.523413\n",
      "Batch: 701 loss: 2.912928\n",
      "Batch: 801 loss: 2.647342\n",
      "Batch: 901 loss: 3.015796\n",
      "Batch: 1001 loss: 2.539300\n",
      "epoch 8 loss: 2.774159\n",
      "Batch: 1 loss: 2.813444\n",
      "Batch: 101 loss: 2.176236\n",
      "Batch: 201 loss: 2.632851\n",
      "Batch: 301 loss: 2.560772\n",
      "Batch: 401 loss: 2.600281\n",
      "Batch: 501 loss: 2.754308\n",
      "Batch: 601 loss: 2.383105\n",
      "Batch: 701 loss: 2.757866\n",
      "Batch: 801 loss: 2.481350\n",
      "Batch: 901 loss: 2.894608\n",
      "Batch: 1001 loss: 2.404825\n",
      "epoch 9 loss: 2.663682\n",
      "Batch: 1 loss: 2.755907\n",
      "Batch: 101 loss: 2.055249\n",
      "Batch: 201 loss: 2.573802\n",
      "Batch: 301 loss: 2.508477\n",
      "Batch: 401 loss: 2.531345\n",
      "Batch: 501 loss: 2.666499\n",
      "Batch: 601 loss: 2.296333\n",
      "Batch: 701 loss: 2.621182\n",
      "Batch: 801 loss: 2.445358\n",
      "Batch: 901 loss: 2.815263\n",
      "Batch: 1001 loss: 2.338591\n",
      "epoch 10 loss: 2.574249\n",
      "Batch: 1 loss: 2.643083\n",
      "Batch: 101 loss: 1.955209\n",
      "Batch: 201 loss: 2.455334\n",
      "Batch: 301 loss: 2.454126\n",
      "Batch: 401 loss: 2.448016\n",
      "Batch: 501 loss: 2.558969\n",
      "Batch: 601 loss: 2.274344\n",
      "Batch: 701 loss: 2.555894\n",
      "Batch: 801 loss: 2.285748\n",
      "Batch: 901 loss: 2.694202\n",
      "Batch: 1001 loss: 2.291303\n",
      "epoch 11 loss: 2.494075\n",
      "Batch: 1 loss: 2.564184\n",
      "Batch: 101 loss: 1.907029\n",
      "Batch: 201 loss: 2.384509\n",
      "Batch: 301 loss: 2.393389\n",
      "Batch: 401 loss: 2.382232\n",
      "Batch: 501 loss: 2.540092\n",
      "Batch: 601 loss: 2.167318\n",
      "Batch: 701 loss: 2.523074\n",
      "Batch: 801 loss: 2.239340\n",
      "Batch: 901 loss: 2.669454\n",
      "Batch: 1001 loss: 2.215219\n",
      "epoch 12 loss: 2.426922\n",
      "Batch: 1 loss: 2.497877\n",
      "Batch: 101 loss: 1.822995\n",
      "Batch: 201 loss: 2.289178\n",
      "Batch: 301 loss: 2.335704\n",
      "Batch: 401 loss: 2.349421\n",
      "Batch: 501 loss: 2.498975\n",
      "Batch: 601 loss: 2.123650\n",
      "Batch: 701 loss: 2.444773\n",
      "Batch: 801 loss: 2.192046\n",
      "Batch: 901 loss: 2.580118\n",
      "Batch: 1001 loss: 2.225582\n",
      "epoch 13 loss: 2.366380\n",
      "Batch: 1 loss: 2.445343\n",
      "Batch: 101 loss: 1.755264\n",
      "Batch: 201 loss: 2.236192\n",
      "Batch: 301 loss: 2.307129\n",
      "Batch: 401 loss: 2.299541\n",
      "Batch: 501 loss: 2.409483\n",
      "Batch: 601 loss: 2.022122\n",
      "Batch: 701 loss: 2.355885\n",
      "Batch: 801 loss: 2.145807\n",
      "Batch: 901 loss: 2.570422\n",
      "Batch: 1001 loss: 2.173810\n",
      "epoch 14 loss: 2.309082\n",
      "Batch: 1 loss: 2.369515\n",
      "Batch: 101 loss: 1.670939\n",
      "Batch: 201 loss: 2.200158\n",
      "Batch: 301 loss: 2.313077\n",
      "Batch: 401 loss: 2.261359\n",
      "Batch: 501 loss: 2.322493\n",
      "Batch: 601 loss: 2.014258\n",
      "Batch: 701 loss: 2.358102\n",
      "Batch: 801 loss: 2.105467\n",
      "Batch: 901 loss: 2.554750\n",
      "Batch: 1001 loss: 2.042525\n",
      "epoch 15 loss: 2.260840\n",
      "Batch: 1 loss: 2.308590\n",
      "Batch: 101 loss: 1.634232\n",
      "Batch: 201 loss: 2.147775\n",
      "Batch: 301 loss: 2.228030\n",
      "Batch: 401 loss: 2.173108\n",
      "Batch: 501 loss: 2.302196\n",
      "Batch: 601 loss: 1.990483\n",
      "Batch: 701 loss: 2.261668\n",
      "Batch: 801 loss: 2.058473\n",
      "Batch: 901 loss: 2.462027\n",
      "Batch: 1001 loss: 2.098329\n",
      "epoch 16 loss: 2.214793\n",
      "Batch: 1 loss: 2.240692\n",
      "Batch: 101 loss: 1.558955\n",
      "Batch: 201 loss: 2.090957\n",
      "Batch: 301 loss: 2.221326\n",
      "Batch: 401 loss: 2.160550\n",
      "Batch: 501 loss: 2.269342\n",
      "Batch: 601 loss: 1.976542\n",
      "Batch: 701 loss: 2.237006\n",
      "Batch: 801 loss: 2.038332\n",
      "Batch: 901 loss: 2.425879\n",
      "Batch: 1001 loss: 2.036406\n",
      "epoch 17 loss: 2.177861\n",
      "Batch: 1 loss: 2.253162\n",
      "Batch: 101 loss: 1.560310\n",
      "Batch: 201 loss: 2.099715\n",
      "Batch: 301 loss: 2.176999\n",
      "Batch: 401 loss: 2.174601\n",
      "Batch: 501 loss: 2.262990\n",
      "Batch: 601 loss: 1.882329\n",
      "Batch: 701 loss: 2.207415\n",
      "Batch: 801 loss: 1.947023\n",
      "Batch: 901 loss: 2.313226\n",
      "Batch: 1001 loss: 2.002039\n",
      "epoch 18 loss: 2.150067\n",
      "Batch: 1 loss: 2.219201\n",
      "Batch: 101 loss: 1.490591\n",
      "Batch: 201 loss: 1.987545\n"
     ]
    }
   ],
   "source": [
    "rec_loss = []\n",
    "for e in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "\n",
    "    for b in range(batch_num):\n",
    "        if b % 100 == 1: \n",
    "            print('Batch: %d loss: %f' % (b, batch_loss))\n",
    "        x, y, g, w = batch.get(b)\n",
    "        batch_loss = model.train(x, y, g, w)\n",
    "        train_loss += batch_loss\n",
    "\n",
    "    train_loss /= batch_num\n",
    "    rec_loss.append(train_loss)\n",
    "    print(\"epoch %d loss: %f\" % (e, train_loss))\n",
    "    model.save(e)\n",
    "\n",
    "np.save('./model/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/seq2seq_18.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.restore(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 loss: 1.564755\n",
      "Batch: 101 loss: 1.563944\n",
      "Batch: 201 loss: 2.085399\n",
      "Batch: 301 loss: 2.216991\n",
      "Batch: 401 loss: 2.133068\n",
      "Batch: 501 loss: 2.293040\n",
      "Batch: 601 loss: 1.933262\n",
      "Batch: 701 loss: 2.236406\n",
      "Batch: 801 loss: 1.985712\n",
      "Batch: 901 loss: 2.359137\n",
      "Batch: 1001 loss: 1.948127\n",
      "epoch 18 loss: 2.149125\n",
      "Batch: 1 loss: 2.101591\n",
      "Batch: 101 loss: 1.475576\n",
      "Batch: 201 loss: 2.008351\n",
      "Batch: 301 loss: 2.160506\n",
      "Batch: 401 loss: 2.115754\n",
      "Batch: 501 loss: 2.247978\n",
      "Batch: 601 loss: 1.887059\n",
      "Batch: 701 loss: 2.193770\n",
      "Batch: 801 loss: 1.921431\n",
      "Batch: 901 loss: 2.292850\n",
      "Batch: 1001 loss: 1.949135\n",
      "epoch 19 loss: 2.106405\n",
      "Batch: 1 loss: 2.096280\n",
      "Batch: 101 loss: 1.504730\n",
      "Batch: 201 loss: 2.048798\n",
      "Batch: 301 loss: 2.099875\n",
      "Batch: 401 loss: 2.097867\n",
      "Batch: 501 loss: 2.173342\n",
      "Batch: 601 loss: 1.783840\n",
      "Batch: 701 loss: 2.152656\n",
      "Batch: 801 loss: 1.944306\n",
      "Batch: 901 loss: 2.324838\n",
      "Batch: 1001 loss: 1.905224\n",
      "epoch 20 loss: 2.081011\n",
      "Batch: 1 loss: 2.074074\n",
      "Batch: 101 loss: 1.471436\n",
      "Batch: 201 loss: 2.003394\n",
      "Batch: 301 loss: 2.087213\n",
      "Batch: 401 loss: 2.074804\n",
      "Batch: 501 loss: 2.137029\n",
      "Batch: 601 loss: 1.832364\n",
      "Batch: 701 loss: 2.103636\n",
      "Batch: 801 loss: 1.857183\n",
      "Batch: 901 loss: 2.280061\n",
      "Batch: 1001 loss: 1.873892\n",
      "epoch 21 loss: 2.057936\n",
      "Batch: 1 loss: 2.020791\n",
      "Batch: 101 loss: 1.437989\n",
      "Batch: 201 loss: 1.981039\n",
      "Batch: 301 loss: 2.100251\n",
      "Batch: 401 loss: 2.033877\n",
      "Batch: 501 loss: 2.154119\n",
      "Batch: 601 loss: 1.782450\n",
      "Batch: 701 loss: 2.091897\n",
      "Batch: 801 loss: 1.854608\n",
      "Batch: 901 loss: 2.248913\n",
      "Batch: 1001 loss: 1.887266\n",
      "epoch 22 loss: 2.029968\n",
      "Batch: 1 loss: 2.022922\n",
      "Batch: 101 loss: 1.459328\n",
      "Batch: 201 loss: 1.909184\n",
      "Batch: 301 loss: 2.088102\n",
      "Batch: 401 loss: 2.001322\n",
      "Batch: 501 loss: 2.131007\n",
      "Batch: 601 loss: 1.767666\n",
      "Batch: 701 loss: 2.110174\n",
      "Batch: 801 loss: 1.820699\n",
      "Batch: 901 loss: 2.253322\n",
      "Batch: 1001 loss: 1.888536\n",
      "epoch 23 loss: 2.009268\n",
      "Batch: 1 loss: 2.040446\n",
      "Batch: 101 loss: 1.416603\n",
      "Batch: 201 loss: 1.917560\n",
      "Batch: 301 loss: 2.082670\n",
      "Batch: 401 loss: 1.979366\n",
      "Batch: 501 loss: 2.074813\n",
      "Batch: 601 loss: 1.744548\n",
      "Batch: 701 loss: 2.067003\n",
      "Batch: 801 loss: 1.847071\n",
      "Batch: 901 loss: 2.245141\n",
      "Batch: 1001 loss: 1.850070\n",
      "epoch 24 loss: 1.994427\n",
      "Batch: 1 loss: 2.001917\n",
      "Batch: 101 loss: 1.411937\n",
      "Batch: 201 loss: 1.875749\n",
      "Batch: 301 loss: 2.047877\n",
      "Batch: 401 loss: 1.955906\n",
      "Batch: 501 loss: 2.058339\n",
      "Batch: 601 loss: 1.774607\n",
      "Batch: 701 loss: 1.997903\n",
      "Batch: 801 loss: 1.772964\n",
      "Batch: 901 loss: 2.227286\n",
      "Batch: 1001 loss: 1.839247\n",
      "epoch 25 loss: 1.963694\n",
      "Batch: 1 loss: 1.967238\n",
      "Batch: 101 loss: 1.388420\n",
      "Batch: 201 loss: 1.869511\n",
      "Batch: 301 loss: 2.012768\n",
      "Batch: 401 loss: 1.932311\n",
      "Batch: 501 loss: 2.042869\n",
      "Batch: 601 loss: 1.761573\n",
      "Batch: 701 loss: 2.007976\n",
      "Batch: 801 loss: 1.811064\n",
      "Batch: 901 loss: 2.150222\n",
      "Batch: 1001 loss: 1.787701\n",
      "epoch 26 loss: 1.944031\n",
      "Batch: 1 loss: 1.910267\n",
      "Batch: 101 loss: 1.346270\n",
      "Batch: 201 loss: 1.807377\n",
      "Batch: 301 loss: 1.985137\n",
      "Batch: 401 loss: 1.934070\n",
      "Batch: 501 loss: 2.087930\n",
      "Batch: 601 loss: 1.695237\n",
      "Batch: 701 loss: 2.041095\n",
      "Batch: 801 loss: 1.797040\n",
      "Batch: 901 loss: 2.122423\n",
      "Batch: 1001 loss: 1.793629\n",
      "epoch 27 loss: 1.934623\n",
      "Batch: 1 loss: 1.967373\n",
      "Batch: 101 loss: 1.314749\n",
      "Batch: 201 loss: 1.847053\n",
      "Batch: 301 loss: 1.995474\n",
      "Batch: 401 loss: 1.887297\n",
      "Batch: 501 loss: 2.011092\n",
      "Batch: 601 loss: 1.687842\n",
      "Batch: 701 loss: 2.001303\n",
      "Batch: 801 loss: 1.762325\n",
      "Batch: 901 loss: 2.201973\n",
      "Batch: 1001 loss: 1.793702\n",
      "epoch 28 loss: 1.926827\n",
      "Batch: 1 loss: 1.960335\n",
      "Batch: 101 loss: 1.344943\n",
      "Batch: 201 loss: 1.808929\n",
      "Batch: 301 loss: 1.981638\n",
      "Batch: 401 loss: 1.851923\n",
      "Batch: 501 loss: 1.981141\n",
      "Batch: 601 loss: 1.625167\n",
      "Batch: 701 loss: 1.965007\n",
      "Batch: 801 loss: 1.704159\n",
      "Batch: 901 loss: 2.116958\n",
      "Batch: 1001 loss: 1.830339\n",
      "epoch 29 loss: 1.893948\n",
      "Batch: 1 loss: 1.921024\n",
      "Batch: 101 loss: 1.284196\n",
      "Batch: 201 loss: 1.767921\n",
      "Batch: 301 loss: 2.024582\n",
      "Batch: 401 loss: 1.868983\n",
      "Batch: 501 loss: 2.024350\n",
      "Batch: 601 loss: 1.658337\n",
      "Batch: 701 loss: 2.030930\n",
      "Batch: 801 loss: 1.730057\n",
      "Batch: 901 loss: 2.109531\n",
      "Batch: 1001 loss: 1.763609\n",
      "epoch 30 loss: 1.894972\n",
      "Batch: 1 loss: 1.912277\n",
      "Batch: 101 loss: 1.288603\n",
      "Batch: 201 loss: 1.785753\n",
      "Batch: 301 loss: 1.957220\n",
      "Batch: 401 loss: 1.872808\n",
      "Batch: 501 loss: 2.000322\n",
      "Batch: 601 loss: 1.627959\n",
      "Batch: 701 loss: 1.927769\n",
      "Batch: 801 loss: 1.705533\n",
      "Batch: 901 loss: 2.053555\n",
      "Batch: 1001 loss: 1.748360\n",
      "epoch 31 loss: 1.883612\n",
      "Batch: 1 loss: 1.896458\n",
      "Batch: 101 loss: 1.288876\n",
      "Batch: 201 loss: 1.781259\n",
      "Batch: 301 loss: 1.915135\n",
      "Batch: 401 loss: 1.899337\n",
      "Batch: 501 loss: 1.953870\n",
      "Batch: 601 loss: 1.631148\n",
      "Batch: 701 loss: 1.984204\n",
      "Batch: 801 loss: 1.743676\n",
      "Batch: 901 loss: 2.033817\n",
      "Batch: 1001 loss: 1.717195\n",
      "epoch 32 loss: 1.868854\n",
      "Batch: 1 loss: 1.827688\n",
      "Batch: 101 loss: 1.336300\n",
      "Batch: 201 loss: 1.799830\n",
      "Batch: 301 loss: 1.946191\n",
      "Batch: 401 loss: 1.872908\n",
      "Batch: 501 loss: 1.955838\n",
      "Batch: 601 loss: 1.560835\n",
      "Batch: 701 loss: 1.919850\n",
      "Batch: 801 loss: 1.688989\n",
      "Batch: 901 loss: 2.018106\n",
      "Batch: 1001 loss: 1.721174\n",
      "epoch 33 loss: 1.856391\n",
      "Batch: 1 loss: 1.872704\n",
      "Batch: 101 loss: 1.240446\n",
      "Batch: 201 loss: 1.727686\n",
      "Batch: 301 loss: 1.913441\n",
      "Batch: 401 loss: 1.832198\n",
      "Batch: 501 loss: 1.928849\n",
      "Batch: 601 loss: 1.518791\n",
      "Batch: 701 loss: 1.922262\n",
      "Batch: 801 loss: 1.665000\n",
      "Batch: 901 loss: 2.041464\n",
      "Batch: 1001 loss: 1.724308\n",
      "epoch 34 loss: 1.839513\n",
      "Batch: 1 loss: 1.879938\n",
      "Batch: 101 loss: 1.277355\n",
      "Batch: 201 loss: 1.785160\n",
      "Batch: 301 loss: 1.895589\n",
      "Batch: 401 loss: 1.849920\n",
      "Batch: 501 loss: 1.897694\n",
      "Batch: 601 loss: 1.559676\n",
      "Batch: 701 loss: 1.849736\n",
      "Batch: 801 loss: 1.610826\n",
      "Batch: 901 loss: 2.029503\n",
      "Batch: 1001 loss: 1.707475\n",
      "epoch 35 loss: 1.826386\n",
      "Batch: 1 loss: 1.859116\n",
      "Batch: 101 loss: 1.194921\n",
      "Batch: 201 loss: 1.742570\n",
      "Batch: 301 loss: 1.879365\n",
      "Batch: 401 loss: 1.832949\n",
      "Batch: 501 loss: 1.942912\n",
      "Batch: 601 loss: 1.537571\n",
      "Batch: 701 loss: 1.930926\n",
      "Batch: 801 loss: 1.674664\n",
      "Batch: 901 loss: 2.008124\n",
      "Batch: 1001 loss: 1.717352\n",
      "epoch 36 loss: 1.823962\n",
      "Batch: 1 loss: 1.833634\n",
      "Batch: 101 loss: 1.256217\n",
      "Batch: 201 loss: 1.720699\n",
      "Batch: 301 loss: 1.927604\n",
      "Batch: 401 loss: 1.781123\n",
      "Batch: 501 loss: 1.888037\n",
      "Batch: 601 loss: 1.520183\n",
      "Batch: 701 loss: 1.845181\n",
      "Batch: 801 loss: 1.691869\n",
      "Batch: 901 loss: 2.056575\n",
      "Batch: 1001 loss: 1.632084\n",
      "epoch 37 loss: 1.818484\n",
      "Batch: 1 loss: 1.829621\n",
      "Batch: 101 loss: 1.264561\n",
      "Batch: 201 loss: 1.740690\n",
      "Batch: 301 loss: 1.910777\n",
      "Batch: 401 loss: 1.830882\n",
      "Batch: 501 loss: 1.946405\n",
      "Batch: 601 loss: 1.558205\n",
      "Batch: 701 loss: 1.905814\n",
      "Batch: 801 loss: 1.662201\n",
      "Batch: 901 loss: 2.035727\n",
      "Batch: 1001 loss: 1.651766\n",
      "epoch 38 loss: 1.816482\n",
      "Batch: 1 loss: 1.825663\n",
      "Batch: 101 loss: 1.186774\n",
      "Batch: 201 loss: 1.719074\n",
      "Batch: 301 loss: 1.865006\n",
      "Batch: 401 loss: 1.776492\n",
      "Batch: 501 loss: 1.916322\n",
      "Batch: 601 loss: 1.557145\n",
      "Batch: 701 loss: 1.877171\n",
      "Batch: 801 loss: 1.666660\n",
      "Batch: 901 loss: 2.027234\n",
      "Batch: 1001 loss: 1.629597\n",
      "epoch 39 loss: 1.792429\n",
      "Batch: 1 loss: 1.828714\n",
      "Batch: 101 loss: 1.198733\n",
      "Batch: 201 loss: 1.758726\n",
      "Batch: 301 loss: 1.870597\n",
      "Batch: 401 loss: 1.788752\n",
      "Batch: 501 loss: 1.906250\n",
      "Batch: 601 loss: 1.533250\n",
      "Batch: 701 loss: 1.855204\n",
      "Batch: 801 loss: 1.599813\n",
      "Batch: 901 loss: 1.928229\n",
      "Batch: 1001 loss: 1.651084\n",
      "epoch 40 loss: 1.797246\n",
      "Batch: 1 loss: 1.808851\n",
      "Batch: 101 loss: 1.210259\n",
      "Batch: 201 loss: 1.726943\n",
      "Batch: 301 loss: 1.871686\n",
      "Batch: 401 loss: 1.825859\n",
      "Batch: 501 loss: 1.886978\n",
      "Batch: 601 loss: 1.537659\n",
      "Batch: 701 loss: 1.809902\n",
      "Batch: 801 loss: 1.636749\n",
      "Batch: 901 loss: 1.968956\n",
      "Batch: 1001 loss: 1.672334\n",
      "epoch 41 loss: 1.792514\n",
      "Batch: 1 loss: 1.798015\n",
      "Batch: 101 loss: 1.283618\n",
      "Batch: 201 loss: 1.629072\n",
      "Batch: 301 loss: 1.850714\n",
      "Batch: 401 loss: 1.783501\n",
      "Batch: 501 loss: 1.874898\n",
      "Batch: 601 loss: 1.572529\n",
      "Batch: 701 loss: 1.834866\n",
      "Batch: 801 loss: 1.631057\n",
      "Batch: 901 loss: 1.936928\n",
      "Batch: 1001 loss: 1.618929\n",
      "epoch 42 loss: 1.768859\n",
      "Batch: 1 loss: 1.747747\n",
      "Batch: 101 loss: 1.158853\n",
      "Batch: 201 loss: 1.657276\n",
      "Batch: 301 loss: 1.896973\n",
      "Batch: 401 loss: 1.807138\n",
      "Batch: 501 loss: 1.912916\n",
      "Batch: 601 loss: 1.514863\n",
      "Batch: 701 loss: 1.819722\n",
      "Batch: 801 loss: 1.538626\n",
      "Batch: 901 loss: 1.868517\n",
      "Batch: 1001 loss: 1.616284\n",
      "epoch 43 loss: 1.749358\n",
      "Batch: 1 loss: 1.802879\n",
      "Batch: 101 loss: 1.140703\n",
      "Batch: 201 loss: 1.677635\n",
      "Batch: 301 loss: 1.879450\n",
      "Batch: 401 loss: 1.762351\n"
     ]
    }
   ],
   "source": [
    "rec_loss = []\n",
    "for e in range(19, EPOCHS):\n",
    "    train_loss = 0\n",
    "\n",
    "    for b in range(batch_num):\n",
    "        if b % 100 == 1: \n",
    "            print('Batch: %d loss: %f' % (b, batch_loss))\n",
    "        x, y, g, w = batch.get(b)\n",
    "        batch_loss = model.train(x, y, g, w)\n",
    "        train_loss += batch_loss\n",
    "\n",
    "    train_loss /= batch_num\n",
    "    rec_loss.append(train_loss)\n",
    "    print(\"epoch %d loss: %f\" % (e, train_loss))\n",
    "    model.save(e)\n",
    "\n",
    "np.save('./model/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/seq2seq_43.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.restore(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 loss: 1.747747\n",
      "Batch: 101 loss: 1.158854\n",
      "Batch: 201 loss: 1.657402\n",
      "Batch: 301 loss: 1.890517\n",
      "Batch: 401 loss: 1.798719\n",
      "Batch: 501 loss: 1.888305\n",
      "Batch: 601 loss: 1.490829\n",
      "Batch: 701 loss: 1.820071\n",
      "Batch: 801 loss: 1.559195\n",
      "Batch: 901 loss: 1.868362\n",
      "Batch: 1001 loss: 1.612518\n",
      "epoch 43 loss: 1.745909\n",
      "Batch: 1 loss: 1.777535\n",
      "Batch: 101 loss: 1.150358\n",
      "Batch: 201 loss: 1.660801\n",
      "Batch: 301 loss: 1.888675\n",
      "Batch: 401 loss: 1.744602\n",
      "Batch: 501 loss: 1.902653\n",
      "Batch: 601 loss: 1.546380\n",
      "Batch: 701 loss: 1.840378\n",
      "Batch: 801 loss: 1.554497\n",
      "Batch: 901 loss: 1.928380\n",
      "Batch: 1001 loss: 1.595371\n",
      "epoch 44 loss: 1.752835\n",
      "Batch: 1 loss: 1.737133\n",
      "Batch: 101 loss: 1.155712\n",
      "Batch: 201 loss: 1.623110\n",
      "Batch: 301 loss: 1.894664\n",
      "Batch: 401 loss: 1.742445\n",
      "Batch: 501 loss: 1.869797\n",
      "Batch: 601 loss: 1.504197\n",
      "Batch: 701 loss: 1.830243\n",
      "Batch: 801 loss: 1.557066\n",
      "Batch: 901 loss: 1.875733\n",
      "Batch: 1001 loss: 1.602853\n",
      "epoch 45 loss: 1.761135\n"
     ]
    }
   ],
   "source": [
    "rec_loss = []\n",
    "for e in range(44, EPOCHS):\n",
    "    train_loss = 0\n",
    "\n",
    "    for b in range(batch_num):\n",
    "        if b % 100 == 1: \n",
    "            print('Batch: %d loss: %f' % (b, batch_loss))\n",
    "        x, y, g, w = batch.get(b)\n",
    "        batch_loss = model.train(x, y, g, w)\n",
    "        train_loss += batch_loss\n",
    "\n",
    "    train_loss /= batch_num\n",
    "    rec_loss.append(train_loss)\n",
    "    print(\"epoch %d loss: %f\" % (e, train_loss))\n",
    "    model.save(e)\n",
    "\n",
    "np.save('./model/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/seq2seq_45.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.restore(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 loss: 1.737133\n",
      "Batch: 101 loss: 1.155712\n",
      "Batch: 201 loss: 1.623049\n",
      "Batch: 301 loss: 1.895772\n",
      "Batch: 401 loss: 1.714554\n",
      "Batch: 501 loss: 1.886192\n",
      "Batch: 601 loss: 1.481389\n",
      "Batch: 701 loss: 1.788327\n",
      "Batch: 801 loss: 1.529632\n",
      "Batch: 901 loss: 1.924595\n",
      "Batch: 1001 loss: 1.613624\n",
      "epoch 46 loss: 1.755118\n",
      "Batch: 1 loss: 1.737347\n",
      "Batch: 101 loss: 1.154730\n",
      "Batch: 201 loss: 1.613453\n",
      "Batch: 301 loss: 1.866787\n",
      "Batch: 401 loss: 1.759150\n",
      "Batch: 501 loss: 1.867481\n",
      "Batch: 601 loss: 1.478531\n",
      "Batch: 701 loss: 1.759460\n",
      "Batch: 801 loss: 1.588596\n",
      "Batch: 901 loss: 1.888155\n",
      "Batch: 1001 loss: 1.638176\n",
      "epoch 47 loss: 1.741701\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d234423679f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrec_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch %d loss: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/rec_loss.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-872058657fd8>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model/seq2seq_%d.ckpt'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1432\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rec_loss = []\n",
    "for e in range(46, EPOCHS):\n",
    "    train_loss = 0\n",
    "\n",
    "    for b in range(batch_num):\n",
    "        if b % 100 == 1: \n",
    "            print('Batch: %d loss: %f' % (b, batch_loss))\n",
    "        x, y, g, w = batch.get(b)\n",
    "        batch_loss = model.train(x, y, g, w)\n",
    "        train_loss += batch_loss\n",
    "\n",
    "    train_loss /= batch_num\n",
    "    rec_loss.append(train_loss)\n",
    "    print(\"epoch %d loss: %f\" % (e, train_loss))\n",
    "    model.save(e)\n",
    "\n",
    "np.save('./model/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/seq2seq_45.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.restore(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Hello. <br>\n",
    "How are you? <br>\n",
    "Where are you going? <br>\n",
    "You look great. <br>\n",
    "Good night. <br>\n",
    "\n",
    "1. 將以上五個問題先經過 filter_line() 前處理，將大寫轉換成小寫、過濾掉標點符號等\n",
    "2. 每句前面加上SOS，句尾加上EOS，沒看過的字用UNK取代\n",
    "3. 轉換成 index\n",
    "4. 用前面的 BatchGenerator() 包成 batch\n",
    "5. 用cherry pick挑出最好的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "\n",
      "how are you\n",
      "fine\n",
      "\n",
      "[0, 183, 1]\n",
      "[0, 183, 1]\n",
      "\n",
      "[0, 23, 31, 15, 1]\n",
      "[0, 477, 1]\n"
     ]
    }
   ],
   "source": [
    "testing_questions = ['Hi',\n",
    "                     'How are you?',\n",
    "                     'Where are you going?',\n",
    "                     'You look great.',\n",
    "                     'Good night.']\n",
    "testing_answers = ['Hi',\n",
    "                   'fine',\n",
    "                   'I am going to ',\n",
    "                   'Thanks',\n",
    "                   'Good night']\n",
    "\n",
    "testing_questions_filtered = []\n",
    "testing_answers_filtered = []\n",
    "for question, answer in zip(testing_questions, testing_answers):\n",
    "    tmp = filter_line(question)\n",
    "    testing_questions_filtered.append(tmp)\n",
    "    tmp = filter_line(answer)\n",
    "    testing_answers_filtered.append(tmp)\n",
    "    \n",
    "print(testing_questions_filtered[0])\n",
    "print(testing_answers_filtered[0])\n",
    "print()\n",
    "print(testing_questions_filtered[1])\n",
    "print(testing_answers_filtered[1])\n",
    "print()\n",
    "\n",
    "testing_Q = []\n",
    "testing_A = []\n",
    "\n",
    "for question, answer in zip(testing_questions_filtered, testing_answers_filtered):\n",
    "    tmp_q = []\n",
    "    tmp_a = []\n",
    "    tmp_q.append(word2index['<SOS>'])\n",
    "    tmp_a.append(word2index['<SOS>'])\n",
    "    for q in question.split():\n",
    "        if q not in word2index:\n",
    "            tmp_q.append(word2index['<UNK>'])\n",
    "        else:\n",
    "            tmp_q.append(word2index[q])\n",
    "    for a in answer.split():\n",
    "        if a not in word2index:\n",
    "            tmp_a.append(word2index['<UNK>'])\n",
    "        else:\n",
    "            tmp_a.append(word2index[a])\n",
    "\n",
    "    tmp_q.append(word2index['<EOS>'])\n",
    "    tmp_a.append(word2index['<EOS>'])\n",
    "    testing_Q.append(tmp_q)\n",
    "    testing_A.append(tmp_a)\n",
    "\n",
    "print(testing_Q[0])\n",
    "print(testing_A[0])\n",
    "print()\n",
    "print(testing_Q[1])\n",
    "print(testing_A[1])\n",
    "\n",
    "batch_test = BatchGenerator(testing_Q, testing_A, word2index['<PAD>'], word2index['<PAD>'], q_max_len, a_max_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def cherry_pick(records, n, upper_bound=1.0):\n",
    "    bleus = []\n",
    "    \n",
    "    for en, ch_gr, ch_pd in records:\n",
    "        # caculate BLEU by nltk\n",
    "        bleu = nltk.translate.bleu_score.sentence_bleu([ch_gr], ch_pd)\n",
    "        bleus.append(bleu)\n",
    "    \n",
    "    lst = [i for i in range(len(records)) if bleus[i]<=upper_bound]\n",
    "    lst = sorted(lst, key=lambda i: bleus[i], reverse=True) # sort by BLEU score\n",
    "    \n",
    "    return [records[lst[i]] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good night\n",
      "good night\n",
      "\n",
      "where are you going\n",
      "to complete my cure\n",
      "\n",
      "how are you\n",
      "fine i will take a minute\n",
      "\n",
      "hi\n",
      "i am scared\n",
      "\n",
      "you look great\n",
      "i am not sure you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "\n",
    "records = []\n",
    "\n",
    "for i in range(5):\n",
    "    x, y, g, w = batch_test.get(i)\n",
    "    out = model.output(x, y)\n",
    "    pd = model.predict(x, word2index['<SOS>'])\n",
    "    xs = [index2word[x[i][0]] for i in range(q_max_len)]\n",
    "    xs = xs[:xs.index('<PAD>')]\n",
    "    ys_gr = [index2word[g[i][0]] for i in range(a_max_len)]\n",
    "    if '<EOS>' in ys_gr:\n",
    "        ys_gr = ys_gr[:ys_gr.index('<EOS>')]\n",
    "    ys_pd = [index2word[np.argmax(pd[i][0, :])] for i in range(a_max_len)]\n",
    "    if '<EOS>' in ys_pd:\n",
    "        ys_pd = ys_pd[:ys_pd.index('<EOS>')]\n",
    "\n",
    "    records.append([xs, ys_gr, ys_pd])\n",
    "\n",
    "n = 5  # how many result we show\n",
    "rec_cherry = cherry_pick(records, n)\n",
    "\n",
    "for i in range(n):\n",
    "    print(' '.join(rec_cherry[i][0]))\n",
    "    print(' '.join(rec_cherry[i][2]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析\n",
    "\n",
    "loss從最一開始的9點多，經過50個epoch降到1點多，因此應該是有train起來的。從印出的loss中可以看出來它還在繼續收斂，如果有更多時間的話應該有機會訓練出更好的model。我覺得我的 model 跟我平常講話的習慣還滿相似的，以下是我為每段對話想的一些情境\n",
    "\n",
    "1. 第一組是正常的問答，A：晚安，B：說晚安\n",
    "2. 第二組也相當符合常理，B可能在去醫院的路上，A：你要去哪？ B：我要去完成療程\n",
    "3. 第三組是日常對話，A, B在路上偶遇，A先跟B打招呼說「你好嗎？」，但 B 忙著講電話，因此就匆匆說「還行，等我一分鐘」\n",
    "4. 第四組對話的情境可能是 A 在路上看到 B 就跟他打招呼說「嗨」，但 B 當時在恍神，所以就說「嚇我一跳」\n",
    "5. 第五組對話常常發生在我們宿舍，A 說「你看起來真美」，B 就會說「我才不相信你哈哈哈」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
